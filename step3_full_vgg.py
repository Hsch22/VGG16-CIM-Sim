"""
å…¨ç½‘ç»œ CIM è½¬æ¢ä¸éªŒè¯ã€‚

æœ¬æ¨¡å—å°†æ ‡å‡† VGG16 æ¨¡å‹è½¬æ¢ä¸ºåŸºäº CIM çš„å®ç°ã€‚
æ ¸å¿ƒä»»åŠ¡åŒ…æ‹¬ï¼š
- CIM_Linear: å…¨è¿æ¥å±‚çš„ CIM å®ç°ã€‚
- VGG16_CIM: å®Œæ•´çš„ VGG16 æ¨¡å‹ç»„è£…ï¼Œå°† Conv2d å’Œ Linear æ›¿æ¢ä¸º CIM ç‰ˆæœ¬ã€‚
- ç«¯åˆ°ç«¯éªŒè¯: éªŒè¯è½¬æ¢åçš„ VGG16 ä¸æ ‡å‡† VGG16 åœ¨æ¨ç†ç»“æœä¸Šçš„ä¸€è‡´æ€§ã€‚

éçŸ©é˜µè®¡ç®—å±‚ï¼ˆå¦‚ ReLU, MaxPool, AvgPool, Dropoutï¼‰ç›´æ¥å¤ç”¨ PyTorch åŸç”Ÿå®ç°ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models


class CIM_Tile(nn.Module):
    """
    æ¨¡æ‹Ÿå­˜å†…è®¡ç®— (CIM) çš„åŸºç¡€é˜µåˆ—æ¨¡å—ã€‚

    è¯¥æ¨¡å—æ˜¯ä» Step 2 ä¸­å¤ç”¨çš„åŸºç¡€ç»„ä»¶ã€‚
    """

    def __init__(self, in_features, out_features, bias=True):
        """
        åˆå§‹åŒ– CIM é˜µåˆ—ã€‚

        Args:
            in_features (int): è¾“å…¥ç‰¹å¾ç»´åº¦ã€‚
            out_features (int): è¾“å‡ºç‰¹å¾ç»´åº¦ã€‚
            bias (bool): æ˜¯å¦åŒ…å«åç½®ã€‚
        """
        super().__init__()
        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)

    def load_weights(self, original_weight, original_bias=None):
        """
        åŠ è½½æƒé‡ã€‚

        å°†å·ç§¯æ ¸æˆ–å…¨è¿æ¥å±‚çš„æƒé‡ç»Ÿä¸€å±•å¹³ä¸º 2D çŸ©é˜µå­˜å‚¨ã€‚
        """
        with torch.no_grad():
            self.weight.copy_(original_weight.view(original_weight.shape[0], -1))
            if original_bias is not None:
                self.bias.copy_(original_bias)

    def forward(self, x_vector):
        """
        æ‰§è¡ŒçŸ©é˜µä¹˜æ³• (Input @ Weight.T + Bias)ã€‚
        """
        return F.linear(x_vector, self.weight, self.bias)


class CIM_Conv2d(nn.Module):
    """
    åŸºäº CIM é˜µåˆ—çš„ 2D å·ç§¯å±‚ã€‚
    """

    def __init__(
        self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True
    ):
        super().__init__()
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.out_channels = out_channels
        self.input_dim = in_channels * kernel_size * kernel_size
        self.cim_tile = CIM_Tile(self.input_dim, out_channels, bias)

    def forward(self, x):
        n, c, h, w = x.shape

        # im2col: å±•å¼€è¾“å…¥å›¾åƒ
        x_unfolded = F.unfold(
            x, kernel_size=self.kernel_size, padding=self.padding, stride=self.stride
        )
        x_col = x_unfolded.transpose(1, 2)  # [N, L, Input_Dim]

        # CIM è®¡ç®—: çŸ©é˜µä¹˜æ³•
        out_col = self.cim_tile(x_col)

        # col2im: æ¢å¤ç©ºé—´ç»“æ„
        h_out = (h + 2 * self.padding - self.kernel_size) // self.stride + 1
        w_out = (w + 2 * self.padding - self.kernel_size) // self.stride + 1
        out = out_col.transpose(1, 2).view(n, self.out_channels, h_out, w_out)
        return out


class CIM_Linear(nn.Module):
    """
    åŸºäº CIM é˜µåˆ—çš„å…¨è¿æ¥å±‚ (Linear Layer)ã€‚
    """

    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        # å…¨è¿æ¥å±‚æœ¬è´¨å°±æ˜¯ä¸€æ¬¡çŸ©é˜µä¹˜æ³•ï¼Œç›´æ¥å¤ç”¨ CIM_Tile
        self.cim_tile = CIM_Tile(in_features, out_features, bias)

    def forward(self, x):
        """
        å…¨è¿æ¥å±‚å‰å‘ä¼ æ’­ã€‚

        Args:
            x (Tensor): è¾“å…¥å‘é‡ï¼Œå½¢çŠ¶ [Batch, In_Features]ã€‚
        """
        return self.cim_tile(x)


class VGG16_CIM(nn.Module):
    """
    CIM ç‰ˆ VGG16 æ¨¡å‹ã€‚

    è‡ªåŠ¨éå†æ ‡å‡† VGG16 æ¨¡å‹ï¼Œå°† Conv2d å’Œ Linear å±‚æ›¿æ¢ä¸ºå¯¹åº”çš„ CIM å®ç°ï¼Œ
    å¹¶ä¿ç•™åŸæœ‰çš„ ReLU, MaxPool ç­‰å±‚ã€‚
    """

    def __init__(self, original_model):
        """
        åˆå§‹åŒ– CIM ç‰ˆ VGG16ã€‚

        Args:
            original_model (nn.Module): é¢„è®­ç»ƒçš„æ ‡å‡† VGG16 æ¨¡å‹ã€‚
        """
        super().__init__()

        # è½¬æ¢ç‰¹å¾æå–éƒ¨åˆ† (Features)
        self.features = nn.ModuleList()
        for layer in original_model.features:
            if isinstance(layer, nn.Conv2d):
                # é‡åˆ°å·ç§¯å±‚ï¼Œæ›¿æ¢ä¸º CIM_Conv2d
                new_layer = CIM_Conv2d(
                    layer.in_channels,
                    layer.out_channels,
                    layer.kernel_size[0],
                    layer.stride[0],
                    layer.padding[0],
                )
                # ç«‹å³ä»åŸå±‚åŠ è½½æƒé‡
                new_layer.cim_tile.load_weights(layer.weight, layer.bias)
                self.features.append(new_layer)
            else:
                # é‡åˆ° ReLU, MaxPool ç­‰ï¼Œç›´æ¥å¤ç”¨åŸå±‚
                self.features.append(layer)

        # ä¿ç•™ AvgPool å±‚
        self.avgpool = original_model.avgpool

        # è½¬æ¢åˆ†ç±»å™¨éƒ¨åˆ† (Classifier)
        self.classifier = nn.ModuleList()
        for layer in original_model.classifier:
            if isinstance(layer, nn.Linear):
                # é‡åˆ°å…¨è¿æ¥å±‚ï¼Œæ›¿æ¢ä¸º CIM_Linear
                new_layer = CIM_Linear(layer.in_features, layer.out_features)
                new_layer.cim_tile.load_weights(layer.weight, layer.bias)
                self.classifier.append(new_layer)
            else:
                # é‡åˆ° Dropout, ReLUï¼Œç›´æ¥å¤ç”¨
                self.classifier.append(layer)

    def forward(self, x):
        # å‰å‘ä¼ æ’­ï¼šä¸²è”ç‰¹å¾æå–å±‚
        for layer in self.features:
            x = layer(x)

        x = self.avgpool(x)
        x = torch.flatten(x, 1)  # å±•å¹³ï¼Œè¿æ¥å·ç§¯éƒ¨åˆ†å’Œå…¨è¿æ¥éƒ¨åˆ†

        # å‰å‘ä¼ æ’­ï¼šä¸²è”åˆ†ç±»å™¨å±‚
        for layer in self.classifier:
            x = layer(x)

        return x


if __name__ == "__main__":
    print("æ­£åœ¨ç»„è£… CIM ç‰ˆ VGG16...")

    # åŠ è½½æ ‡å‡†é¢„è®­ç»ƒæ¨¡å‹
    std_vgg = models.vgg16()
    std_weight_path = ".\\model\\vgg16-397923af.pth"
    std_state_dict = torch.load(
        std_weight_path,
        map_location=torch.device("cuda" if torch.cuda.is_available() else "cpu"),
        weights_only=True,
    )
    std_vgg.load_state_dict(std_state_dict)
    std_vgg.eval()

    # æ„å»º CIM æ¨¡å‹
    my_cim_vgg = VGG16_CIM(std_vgg)
    my_cim_vgg.eval()  # ç¡®ä¿å…³é—­ Dropout ç­‰è®­ç»ƒä¸“ç”¨å±‚

    print("æ¨¡å‹ç»„è£…å®Œæˆã€‚æ­£åœ¨åŠ è½½åŸºå‡†æµ‹è¯•æ•°æ®...")

    # åŠ è½½ Step 1 ç”Ÿæˆçš„æ•°æ®
    try:
        data = torch.load('baseline_data.pth')
        input_tensor = data['input']
        target_output = data['output']

        print("å¼€å§‹æ¨ç†...")
        with torch.no_grad():
            cim_output = my_cim_vgg(input_tensor)

        # ç»“æœæ¯”å¯¹
        # è·å– Top-5 é¢„æµ‹ç»“æœ
        probs = torch.softmax(cim_output[0], dim=0)
        top5_prob, top5_catid = torch.topk(probs, 5)

        print(f"\n--- ç»“æœéªŒè¯ ---")
        print(f"CIM æ¨¡å‹é¢„æµ‹ Top 5: {top5_catid.numpy()}")

        # è®¡ç®—ä¸å®˜æ–¹ç»“æœçš„è¯¯å·®
        diff = (target_output - cim_output).abs().max().item()
        print(f"ä¸å®˜æ–¹æ¨¡å‹çš„æœ€å¤§è¯¯å·®: {diff:.6f}")

        if diff < 1e-4:
            print("\nğŸ‰ å®Œç¾é€šè¿‡ï¼")
        else:
            print("\nâš ï¸ å­˜åœ¨è¯¯å·®ï¼Œè¯·æ£€æŸ¥ Linear å±‚çš„è½¬æ¢æ˜¯å¦æ­£ç¡®ã€‚")

    except FileNotFoundError:
        print("é”™è¯¯ï¼šæ‰¾ä¸åˆ° baseline_data.pthï¼Œè¯·å…ˆè¿è¡Œ Step 1ã€‚")
