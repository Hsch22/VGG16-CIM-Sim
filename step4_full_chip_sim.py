"""
Step 4: èŠ¯ç‰‡çº§å…¨åŠŸèƒ½ä»¿çœŸ (Full Chip Simulation)ã€‚

æœ¬æ¨¡å—å¼•å…¥ç‰©ç†æ„ŸçŸ¥çš„èƒ½è€—æ¨¡å‹å’Œç¡¬ä»¶è¡Œä¸ºæ¨¡æ‹Ÿï¼Œé‡ç‚¹å…³æ³¨ï¼š
- æ—¶å»¶ (Latency): åŸºäºè®¡ç®—å‘¨æœŸæ•°ã€‚
- èƒ½è€— (Energy): åŒ…å«é˜µåˆ—å†…è®¡ç®— (Intra-Array) å’Œé˜µåˆ—é—´ç´¯åŠ  (Inter-Tile) çš„åŠ¨æ€æƒè¡¡ã€‚
- ç²¾åº¦ (Accuracy): å¼•å…¥é‡åŒ– (Quantization) æ¨¡æ‹Ÿã€‚

æ ¸å¿ƒæƒè¡¡é€»è¾‘ï¼š
- é˜µåˆ—è¶Šå¤§ -> å•æ¬¡ MAC èƒ½è€—è¶Šé«˜ (ä½çº¿å¯„ç”Ÿç”µå®¹å¢åŠ )ã€‚
- é˜µåˆ—è¶Šå° -> ç´¯åŠ èƒ½è€—è¶Šé«˜ (éœ€è¦æ›´å¤šéƒ¨åˆ†å’Œç´¯åŠ æ“ä½œ)ã€‚
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.models as models
from dataclasses import dataclass
from collections import defaultdict
import math


@dataclass
class SimConfig:
    """
    ä»¿çœŸé…ç½®å‚æ•°ç±»ã€‚

    ç®¡ç†æ¶æ„å‚æ•°ã€ç²¾åº¦è®¾ç½®å’Œç‰©ç†èƒ½è€—æ¨¡å‹å‚æ•°ã€‚
    """

    # --- æ¶æ„å‚æ•° ---
    cim_array_height: int = 128  # Word Lines (å†³å®šä½çº¿é•¿åº¦/å¯„ç”Ÿç”µå®¹)
    cim_array_width: int = 128  # Bit Lines

    # --- ç²¾åº¦å‚æ•° ---
    enable_quantization: bool = True
    weight_bits: int = 8
    activation_bits: int = 8

    # --- èƒ½è€—å‚æ•° (å•ä½: pJ) ---

    # MAC èƒ½è€—æ¨¡å‹: E_mac = Base + (Height * Scaling)
    # ç‰©ç†è§„å¾‹ï¼šé˜µåˆ—è¶Šå¤§ï¼Œä½çº¿è¶Šé•¿ï¼Œå•æ¬¡è¿ç®—èƒ½è€—è¶Šé«˜
    base_mac_pj: float = 0.05  # æå°é˜µåˆ—çš„åŸºç¡€èƒ½è€—
    cap_scaling_factor: float = 0.0005  # æ¯å¢åŠ ä¸€è¡Œå¸¦æ¥çš„é¢å¤–èƒ½è€— (å¯„ç”Ÿç”µå®¹)

    # æ¥å£èƒ½è€—
    energy_adc_pj: float = 2.0  # ADC è½¬æ¢èƒ½è€—

    # æ•°å­—/ç¼“å­˜èƒ½è€—
    energy_digital_pj: float = 0.05  # æ™®é€šæ•°å­—é€»è¾‘ (ReLU/Pool)
    energy_accum_pj: float = (
        0.5  # éƒ¨åˆ†å’Œç´¯åŠ  (Partial Sum Accumulation): SRAMè¯»å†™ + åŠ æ³•å™¨
    )

    @property
    def dynamic_mac_energy(self):
        """
        è®¡ç®—å½“å‰é˜µåˆ—å°ºå¯¸ä¸‹çš„å•æ¬¡ MAC èƒ½è€—ã€‚

        Returns:
            float: å•æ¬¡ä¹˜åŠ è¿ç®—çš„èƒ½è€— (pJ)ã€‚
        """
        return self.base_mac_pj + (self.cim_array_height * self.cap_scaling_factor)


# å…¨å±€é…ç½®å®ä¾‹
CONFIG = SimConfig()


class StatsRecorder:
    """
    å…¨å±€ç»Ÿè®¡è®°å½•å™¨ã€‚

    ç”¨äºåœ¨æ¨ç†è¿‡ç¨‹ä¸­æ”¶é›†å„å±‚çš„ç¡¬ä»¶ç»Ÿè®¡æ•°æ® (MACs, ADC, Latency ç­‰)ã€‚
    """

    def __init__(self):
        self.reset()

    def reset(self):
        """é‡ç½®æ‰€æœ‰ç»Ÿè®¡æ•°æ®ã€‚"""
        self.layers = defaultdict(
            lambda: {
                "macs": 0,
                "adc_ops": 0,
                "digital_ops": 0,
                "accum_ops": 0,
                "latency_cycles": 0,
            }
        )
        self.current_layer_name = "Unknown"

    def set_layer(self, name):
        """è®¾ç½®å½“å‰æ­£åœ¨è®°å½•çš„å±‚åç§°ã€‚"""
        self.current_layer_name = name

    def add_macs(self, count):
        self.layers[self.current_layer_name]["macs"] += count

    def add_adc(self, count):
        self.layers[self.current_layer_name]["adc_ops"] += count

    def add_digital(self, count):
        self.layers[self.current_layer_name]["digital_ops"] += count

    def add_accum(self, count):
        self.layers[self.current_layer_name]["accum_ops"] += count

    def add_latency(self, cycles):
        self.layers[self.current_layer_name]["latency_cycles"] += cycles


# å…¨å±€ç»Ÿè®¡å®ä¾‹
STATS = StatsRecorder()


def quantize_tensor(x, bits):
    """
    æ¨¡æ‹Ÿé‡åŒ–æ“ä½œã€‚

    Args:
        x (Tensor): è¾“å…¥æµ®ç‚¹å¼ é‡ã€‚
        bits (int): é‡åŒ–ä½å®½ã€‚

    Returns:
        Tensor: é‡åŒ–å¹¶åé‡åŒ–åçš„å¼ é‡ (Simulated Quantization)ã€‚
    """
    if not CONFIG.enable_quantization or bits >= 32:
        return x

    qmin = -(2 ** (bits - 1))
    qmax = (2 ** (bits - 1)) - 1

    # ç®€åŒ– Scale è®¡ç®— (å®é™…éƒ¨ç½²é€šå¸¸ç»Ÿè®¡ Dataset çš„ min/max)
    abs_max = x.abs().max().item()
    if abs_max == 0:
        return x

    scale = abs_max / qmax
    x_int = (x / scale).round().clamp(qmin, qmax)
    x_recon = x_int * scale
    return x_recon


class CIM_Tile_Sim(nn.Module):
    """
    å…·å¤‡ç¡¬ä»¶ä»¿çœŸåŠŸèƒ½çš„ CIM é˜µåˆ—æ¨¡å—ã€‚

    é›†æˆåŠŸèƒ½ï¼š
    - æƒé‡/æ¿€æ´»é‡åŒ–æ¨¡æ‹Ÿã€‚
    - ç¡¬ä»¶æ€§èƒ½ç»Ÿè®¡ (MACs, Latency, Energy)ã€‚
    - é˜µåˆ—æ˜ å°„ (Mapping) ä¸åˆ†å— (Tiling) é€»è¾‘ã€‚
    """

    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        self.in_features = in_features
        self.out_features = out_features

        self.weight = nn.Parameter(torch.Tensor(out_features, in_features))
        if bias:
            self.bias = nn.Parameter(torch.Tensor(out_features))
        else:
            self.register_parameter('bias', None)

    def load_weights(self, original_weight, original_bias=None):
        with torch.no_grad():
            self.weight.copy_(original_weight.view(original_weight.shape[0], -1))
            if original_bias is not None:
                self.bias.copy_(original_bias)

    def forward(self, x_vector):
        """
        æ‰§è¡Œä»¿çœŸå‰å‘ä¼ æ’­ã€‚

        Args:
            x_vector (Tensor): [Batch, Length, In_Features]
        """
        if x_vector.dim() == 2:
            x_vector = x_vector.unsqueeze(1)

        batch_size, length, in_dim = x_vector.shape
        out_dim = self.out_features

        # æ‰§è¡Œè®¡ç®— (æ¨¡æ‹Ÿé‡åŒ–ç²¾åº¦)
        x_q = quantize_tensor(x_vector, CONFIG.activation_bits)
        w_q = quantize_tensor(self.weight, CONFIG.weight_bits)
        output = F.linear(x_q, w_q, self.bias)
        output_q = quantize_tensor(output, CONFIG.activation_bits)

        # æ€§èƒ½ç»Ÿè®¡
        with torch.no_grad():
            # A. åŸºç¡€ MACs
            total_macs = batch_size * length * out_dim * in_dim
            STATS.add_macs(total_macs)

            # B. ADC æ¬¡æ•° (æ¯ä¸ªè¾“å‡ºç‚¹ä¸€æ¬¡)
            total_adcs = batch_size * length * out_dim
            STATS.add_adc(total_adcs)

            # C. é˜µåˆ—æ˜ å°„ä¸åˆ†å— (Mapping & Tiling)
            # è®¡ç®—éœ€è¦æŠŠå¤§çŸ©é˜µåˆ‡æˆå¤šå°‘ä¸ªå°å—æ”¾å…¥ CIM Array
            h_splits = math.ceil(in_dim / CONFIG.cim_array_width)  # è¾“å…¥åˆ‡åˆ†
            v_splits = math.ceil(out_dim / CONFIG.cim_array_height)  # è¾“å‡ºåˆ‡åˆ†

            # D. Latency è®¡ç®—
            # æ—¶é—´æ­¥ = è¾“å…¥å‘é‡æ•° * æƒé‡åˆ†å—æ•°
            tile_ops = h_splits * v_splits
            total_input_vectors = batch_size * length
            total_cycles = total_input_vectors * tile_ops
            STATS.add_latency(total_cycles)

            # E. ç´¯åŠ èƒ½è€—è®¡ç®— (Accumulation Energy)
            # å¦‚æœè¾“å…¥ç»´åº¦è¶…è¿‡é˜µåˆ—å®½åº¦ (h_splits > 1)ï¼Œéœ€è¦è·¨é˜µåˆ—ç´¯åŠ éƒ¨åˆ†å’Œ (Partial Sums)
            # è¿™æ˜¯ä¸€ä¸ªéå¸¸è€—èƒ½çš„æ“ä½œ (SRAM Read/Write + Digital Add)
            # ç´¯åŠ æ¬¡æ•° = æ€»è¾“å‡ºç‚¹æ•° * (åˆ‡åˆ†ä»½æ•° - 1)
            if h_splits > 1:
                total_output_points = batch_size * length * out_dim
                accum_count = total_output_points * (h_splits - 1)
                STATS.add_accum(accum_count)

        return output_q


class CIM_Conv2d_Sim(nn.Module):
    """
    å…·å¤‡ç¡¬ä»¶ä»¿çœŸåŠŸèƒ½çš„ CIM å·ç§¯å±‚ã€‚
    """

    def __init__(
        self, in_channels, out_channels, kernel_size, stride=1, padding=0, bias=True
    ):
        super().__init__()
        self.kernel_size = kernel_size
        self.stride = stride
        self.padding = padding
        self.out_channels = out_channels

        # Conv2d è½¬æ¢ä¸ºçŸ©é˜µä¹˜æ³•: Input Dim = C * K * K
        self.input_dim = in_channels * kernel_size * kernel_size
        self.cim_tile = CIM_Tile_Sim(self.input_dim, out_channels, bias)

    def forward(self, x):
        n, c, h, w = x.shape

        # im2col å¼€é”€
        x_unfolded = F.unfold(
            x, kernel_size=self.kernel_size, padding=self.padding, stride=self.stride
        )
        STATS.add_digital(x_unfolded.numel())  # ç»Ÿè®¡æ¬è¿å¼€é”€

        x_col = x_unfolded.transpose(1, 2)

        # CIM è®¡ç®—
        out_col = self.cim_tile(x_col)

        # col2im å¼€é”€
        h_out = (h + 2 * self.padding - self.kernel_size) // self.stride + 1
        w_out = (w + 2 * self.padding - self.kernel_size) // self.stride + 1
        out = out_col.transpose(1, 2).view(n, self.out_channels, h_out, w_out)
        STATS.add_digital(out.numel())

        return out


class CIM_Linear_Sim(nn.Module):
    """
    å…·å¤‡ç¡¬ä»¶ä»¿çœŸåŠŸèƒ½çš„ CIM å…¨è¿æ¥å±‚ã€‚
    """

    def __init__(self, in_features, out_features, bias=True):
        super().__init__()
        self.cim_tile = CIM_Tile_Sim(in_features, out_features, bias)

    def forward(self, x):
        return self.cim_tile(x)


class Digital_ReLU(nn.Module):
    """æ•°å­— ReLU å±‚ï¼Œç”¨äºç»Ÿè®¡æ•°å­—è®¡ç®—å¼€é”€ã€‚"""

    def forward(self, x):
        STATS.add_digital(x.numel())
        return F.relu(x, inplace=True)


class Digital_MaxPool2d(nn.Module):
    """æ•°å­— MaxPool å±‚ï¼Œç”¨äºç»Ÿè®¡æ•°å­—è®¡ç®—å¼€é”€ã€‚"""

    def __init__(self, kernel_size, stride=2, padding=0):
        super().__init__()
        self.pool = nn.MaxPool2d(kernel_size, stride, padding)

    def forward(self, x):
        out = self.pool(x)
        STATS.add_digital(out.numel() * 4)  # ä¼°ç®—æ¯”è¾ƒæ“ä½œæ¬¡æ•°
        return out


class Digital_Dropout(nn.Module):
    """æ•°å­— Dropout å±‚ (æ¨ç†æ—¶é€ä¼ )ã€‚"""

    def forward(self, x):
        return x


class VGG16_Sim(nn.Module):
    """
    å…¨èŠ¯ç‰‡ VGG16 ä»¿çœŸæ¨¡å‹ã€‚

    æ›¿æ¢æ‰€æœ‰å±‚ä¸ºå…·å¤‡ç»Ÿè®¡åŠŸèƒ½çš„ä»¿çœŸå±‚ã€‚
    """

    def __init__(self, original_model):
        super().__init__()
        self.features = nn.ModuleList()

        # è½¬æ¢ç‰¹å¾æå–éƒ¨åˆ†
        layer_idx = 0
        for layer in original_model.features:
            if isinstance(layer, nn.Conv2d):
                name = f"Conv2d_{layer_idx}"
                new_layer = CIM_Conv2d_Sim(
                    layer.in_channels,
                    layer.out_channels,
                    layer.kernel_size[0],
                    layer.stride[0],
                    layer.padding[0],
                )
                new_layer.cim_tile.load_weights(layer.weight, layer.bias)
                new_layer._cim_name = name
                self.features.append(new_layer)
                layer_idx += 1
            elif isinstance(layer, nn.ReLU):
                name = f"ReLU_{layer_idx}"
                new_layer = Digital_ReLU()
                new_layer._cim_name = name
                self.features.append(new_layer)
            elif isinstance(layer, nn.MaxPool2d):
                name = f"Pool_{layer_idx}"
                new_layer = Digital_MaxPool2d(2, 2)
                new_layer._cim_name = name
                self.features.append(new_layer)
            else:
                self.features.append(layer)

        self.avgpool = original_model.avgpool

        # è½¬æ¢åˆ†ç±»å™¨éƒ¨åˆ†
        self.classifier = nn.ModuleList()
        for i, layer in enumerate(original_model.classifier):
            if isinstance(layer, nn.Linear):
                name = f"Linear_{i}"
                new_layer = CIM_Linear_Sim(layer.in_features, layer.out_features)
                new_layer.cim_tile.load_weights(layer.weight, layer.bias)
                new_layer._cim_name = name
                self.classifier.append(new_layer)
            elif isinstance(layer, nn.ReLU):
                name = f"Cls_ReLU_{i}"
                new_layer = Digital_ReLU()
                new_layer._cim_name = name
                self.classifier.append(new_layer)
            elif isinstance(layer, nn.Dropout):
                name = f"Dropout_{i}"
                new_layer = Digital_Dropout()
                new_layer._cim_name = name
                self.classifier.append(new_layer)
            else:
                self.classifier.append(layer)

    def forward(self, x):
        STATS.reset()
        for layer in self.features:
            name = getattr(layer, "_cim_name", "Unknown")
            STATS.set_layer(name)
            x = layer(x)

        STATS.set_layer("AvgPool")
        x = self.avgpool(x)
        STATS.add_digital(x.numel())
        x = torch.flatten(x, 1)

        for layer in self.classifier:
            name = getattr(layer, "_cim_name", "Unknown")
            STATS.set_layer(name)
            x = layer(x)
        return x


if __name__ == "__main__":

    print("\n[é…ç½®ä»¿çœŸå‚æ•°]")

    CONFIG.cim_array_height = 256
    CONFIG.cim_array_width = 256

    CONFIG.weight_bits = 8
    CONFIG.activation_bits = 8

    # æ‰“å°åŠ¨æ€è®¡ç®—å‡ºçš„ MAC èƒ½è€—
    current_mac_pj = CONFIG.dynamic_mac_energy

    print(f"    - Array Size: {CONFIG.cim_array_height}x{CONFIG.cim_array_width}")
    print(
        f"    - Dynamic MAC Energy: {current_mac_pj:.4f} pJ (Dependent on Array Height)"
    )
    print(
        f"    - Accumulation Energy: {CONFIG.energy_accum_pj} pJ (Inter-tile communication)"
    )

    print("\n[åŠ è½½ä¸è½¬æ¢æ¨¡å‹]")
    try:
        std_vgg = models.vgg16(weights=models.VGG16_Weights.IMAGENET1K_V1)
    except:
        std_vgg = models.vgg16(pretrained=True)

    sim_model = VGG16_Sim(std_vgg)
    sim_model.eval()

    print("\n[è¿è¡Œä»¿çœŸæ¨ç†]")
    # ç”Ÿæˆéšæœºè¾“å…¥ä»£æ›¿çœŸå®æ•°æ®ï¼Œæ–¹ä¾¿ç›´æ¥è¿è¡Œ
    input_tensor = torch.randn(1, 3, 224, 224)

    with torch.no_grad():
        sim_output = sim_model(input_tensor)

    print("\n" + "=" * 85)
    print(
        f"{'Layer Name':<12} | {'MACs':<8} | {'Lat(Cyc)':<10} | {'E_MAC':<8} | {'E_Accum':<8} | {'Tot_E(uJ)':<10}"
    )
    print("-" * 85)

    total_macs = 0
    total_energy_pj = 0
    total_latency = 0

    for layer_name, data in STATS.layers.items():
        if data["macs"] == 0 and data["digital_ops"] == 0:
            continue

        # ä¿®æ­£åçš„æ€»èƒ½è€—è®¡ç®—å…¬å¼
        # 1. MAC èƒ½è€— (åŠ¨æ€: é˜µåˆ—å¤§ -> å•æ¬¡è€—ç”µå¤§)
        e_mac = data["macs"] * current_mac_pj

        # 2. ç´¯åŠ èƒ½è€— (åŠ¨æ€: é˜µåˆ—å° -> åˆ‡åˆ†å¤š -> ç´¯åŠ å¤š -> è€—ç”µå¤§)
        e_accum = data["accum_ops"] * CONFIG.energy_accum_pj

        # 3. å…¶ä»–å›ºå®šèƒ½è€—
        e_adc = data["adc_ops"] * CONFIG.energy_adc_pj
        e_dig = data["digital_ops"] * CONFIG.energy_digital_pj

        layer_energy = e_mac + e_adc + e_dig + e_accum

        total_macs += data["macs"]
        total_energy_pj += layer_energy
        total_latency += data["latency_cycles"]

        if "Conv" in layer_name or "Linear" in layer_name:
            print(
                f"{layer_name:<12} | {data['macs']/1e6:<8.1f} | {data['latency_cycles']:<10} | "
                f"{e_mac/1e6:<8.2f} | {e_accum/1e6:<8.2f} | {layer_energy/1e6:<10.2f}"
            )

    print("=" * 85)
    print(f"\nğŸ“Š èŠ¯ç‰‡æ€»è§ˆ (Array: {CONFIG.cim_array_height}x{CONFIG.cim_array_width}):")
    print(f"    - æ€»å»¶è¿Ÿ (Latency)     : {total_latency} Cycles")
    print(f"    - æ€»èƒ½è€— (Total Energy): {total_energy_pj/1e6:.4f} uJ")

    if CONFIG.cim_array_height <= 64:
        print("ğŸ’¡ å°é˜µåˆ—å¯¼è‡´ 'E_Accum' (ç´¯åŠ èƒ½è€—) è¾ƒé«˜ï¼Œå› ä¸ºåˆ‡åˆ†æ¬¡æ•°å¤šã€‚")
    elif CONFIG.cim_array_height >= 512:
        print("ğŸ’¡ å¤§é˜µåˆ—å¯¼è‡´ 'E_MAC' (è®¡ç®—èƒ½è€—) è¾ƒé«˜ï¼Œå› ä¸ºä½çº¿å¯„ç”Ÿç”µå®¹å¤§ã€‚")
    else:
        print("ğŸ’¡ è¿™æ˜¯ä¸€ä¸ªå¹³è¡¡ç‚¹é…ç½®ã€‚")
